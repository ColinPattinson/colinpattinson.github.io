---
layout: post
title:  "Managing a Live service"
date:   2023-09-21
excerpt: "Hints & tips about the live phase"
tag: [Kry]
comments: false
---
Managing a live service

I’ve been asked to give a little talk about managing a live service. What follows is a list of tips, tricks & activities that spring to mind when being a product manager for a service out in the wild. Needless to say it’s a slightly different mindset then being in [discovery](https://www.gov.uk/service-manual/agile-delivery/how-the-discovery-phase-works), [alpha](https://www.gov.uk/service-manual/agile-delivery/how-the-alpha-phase-works) or even private [beta](https://www.gov.uk/service-manual/agile-delivery/how-the-beta-phase-works).

# What do I mean by a live service
Just for clarity’s sake. Broadly, I subscribe to the notion that a service is [“something that helps someone to do something”](https://good.services/blog/what-is-a-service). For example, this could be applying for a new passport or booking a GP appointment. Your organisation may have bigger goals and outcomes it is seeking but typically most people will be owning some segment of a broader problem/opportunity.

By ‘live’ I mean the service is mostly unrestricted to your target audience. Private beta you’ll likely be throttling users to learn & iterate and still scaling up to allow more people to use your service. ‘Live’ is what comes after. 
I absolutely am not envisioning if you’ve made your service this far that it is a perfect service that fulfils every need of your user. Maybe you’ve not quite nailed product/market fit or some elements of the journey are somewhat wonky. This is fine, while you may wind down the amount of people/investment you might have thrown at the problem initially there is typically still work to be done.

So… with that hopefully clarified. What do I think should be floating in your brain as you sit there with this live service on your hands.

## Does it do the thing it’s supposed to… well?
As stated previously the service will be fulfilling some sort of purpose. It might functionally be able to provide users with what they need but how well does it do it? 

By this point a team should have some form of monitoring on how its service is performing which is going to be key in assessing what might be worth investing more time into it to make it better. Actively you & the team will be routinely monitoring how the service is performing. It could be looking at anything from:
- user conversion rate metrics are lower than industry standard
- a sudden decline in retention rate
- the speed of the service is slow with a big average page load speed
- the uptime of the service is problematic and users couldn’t access the service when they wanted to
- accessibility testing highlights issues for a subset of your users

All of these are valid things to investigate and act upon and in general are healthy signs the team is continually improving the service. These would need to be prioritised against other endeavours.

The team might have some broader OKR’s or KPI’s that the team should be monitoring too (e.g. profitability, uptake, etc) & while their part of the service only contributes a portion to these metrics they should be followed to sniff out potential areas of improvement.

Metrics aren’t everything. It is healthy to continue to amass qualitative evidence on the experience of your service. This will give you a deeper understanding of how users feel & what they do as they progress around the service. It can highlight more areas that need addressing or spark new ideas about what is missing. Perhaps also there user groups the team hadn’t prioritised at first but feel interesting and unique compared to the other group the service primarily is working for so warrants continued effort to know your user. 
There are loads of ways to get these insights from app store reviews, user satisfaction scores/comments, surveys, interviews, ethnographic research. All can be valuable. 

When managing a live service you get the great opportunity to invest more time and effort into knowing your user, from their needs to their patterns of behaviour. Unlike earlier phases when you may have been making more decisions off assumptions, in ‘live’ you likely will have far more expertise than before but staying curious remains important as people change, so will your service need to change with them.

## New endeavours
New goals often spring up. While managing a live service you should still be keeping your ear to the ground within the organisation for new directives that could influence the work you do & the service you are responsible for. For example there could be a new drive to save money/make more profit over growth means you need to look at where you can cut some costs or try to make the service more efficient and cost effective. Or it could be a new partnership is on the horizon so a bunch of new features or [SLA’s](https://en.wikipedia.org/wiki/Service-level_agreement) might land on your plate. Often other teams might also be furiously creating something new too which might require swarming on an endeavour. 
The live service will adapt over time. I’d suggest trying to stay ahead of the pack, especially if you wish to shape what your service becomes.

Often these types of new initiatives can start off quite hazy ideas and for these occasions I’ve found it helpful to try to use [opportunity trees](https://www.producttalk.org/opportunity-solution-tree/) & working with the team in some form of [ideation session](https://designsprintkit.withgoogle.com/methodology/phase3-sketch/crazy-8s) to figure out how your team & service could contribute to any new stated organisational goal. These activities should help with understanding what contributions you could make and the metrics that you can focus on to fix when new things rear their head. Ideally nice & early and pre-emptive. 

Needless to say. You should continue to stay curious and alert to how your service could change and invest time in monitoring its performance and understanding the landscape your service sits in.

## Staying relevant
Very much in keeping with the above, is factoring in that your solution’s ability to satiate a need will degrade over time. A users expectation and need will morph with time and while you may strike gold in your first iteration it’s more than likely you’ll need to morph with what people expect. 

It’s more than likely the earliest version of your service might not “[solve the whole problem](https://www.gov.uk/service-manual/agile-delivery/how-the-live-phase-works#meeting-the-standard-to-move-into-live)” in the first place. Almost certainly extra effort is needed to make sure the service works intuitively across the wider experience. This could be preventing people from filling out the same form multiple times or making sure the service feels consistent no matter where you are within it. Added onto this the organisation or your team most likely will have set a vision that was out of reach on purpose & either needs a refresh or might still have room to grow into depending on how big you dreamed. There are some great resources on hallmarks on what makes a [great service](https://good.services/15-principles-of-good-service-design) which might spawn new ideas on how to improve things.

The other big source of relevancy would be competitors. Other organisations and teams will likely be tackling a similar problem somewhere and provide a great source of inspiration for change. This could be spurred by new technology or just trying to solve the problem in an interesting direction.

Lastly, in knowing your landscape & staying relevant then having a go at [Wardley mapping](https://learnwardleymapping.com/#learn-more) is a sound idea. Effectively a way to map your service with all its component parts and try to predict what is unique about what you offer and thus where you probably can invest most in or cut costs.

## Contracts
This leads me nicely onto managing the service more broadly. Likely you’ll have some contracts or arrangements in place to keep the service up and running. Over time you’ll need to keep track of these deals and try to assess their value against whatever else is on the market or building a solution yourself as a replacement. Switching suppliers can be a long winded process and present a lot of risks so worth keeping on track of what’s in the contract and especially knowing when it's due to end. 

## Making the team hum
With the service now in live you’ll likely have a core team which you can continue to dedicate time to making perform consistently. A lot of trust and expertise would of been built over time but it’s still great to continue doing retrospectives and finding ways the team can become as well oiled a machine as possible.
That also might lead to some team members wanting to spread their wings or try some side hustles they are passionate about. I’ve always been very pro in people to continue to learn & grow and letting people go off to invest time in another endeavour, for a time, is usually holistically good for all. 

There are ways to track how well your team is doing. The team can measure team health in metrics such as:
- routine [team health surveys](https://www.teamretro.com/health-checks/team-health-check) at retrospectives
- Google’s [Elite DevOps metrics](https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance)

All of these are just tools to sense if the team is happy, is it performing as consistently as before and hunt down opportunities to get better.

## Incident management
Being in a live service means at some time, something will go wrong and you & the team will need to deal with it. Over time the team will get better at diagnosing issues but needless to say it can be somewhat stressful when one of the services you own stops working and people are breathing down your neck to get it back up again.

First advice is to try and nail down some sort of process for incidents early on in the team’s history. Firstly, it’s worth having someone ‘on-call’ so if an incident happens people know whom to contact and be responsible for triaging the issue and coming up with what to do next.

Setting up monitoring of your services to see if they are performing as expected and setting up notifications if things wobble so the team can act fast is also very useful. It’s much better to spot issues fast then wait for people to get a bad experience and the organisation to get into a frenzy. 

Next up is a sensible way to diagnose issues when they arise. Simply, not every issue is created equally. Typically some version that means you can rank an issue between:
- urgent: drop everything and fix
- medium urgency: create a bug and fix within the sprint
- non-urgent: create a bug, prioritise later

I would suggest starting with having a matrix that pits how many users are affected vs how big is the impact. For example something urgent would be every user is affected and nobody can pay for their items in their basket. Non-urgent on the other hand could be some devices (<5% of all users) can’t change their language settings in their app. I have found in most companies people can become very animated during incidents and emotions are high but it’s good to not be swept up within the storm and stay true to your process of deciding what really is worth all hands on deck and what is not. 

When the incident strikes it is great to keep people informed. This is also internally making sure that stakeholders are aware and feel confident the incident is being handled (having some #incident channel that people can follow is great) but also making sure users know what is happening. Firstly, it’s a much better experience to communicate well that something is not working quite right but also you don’t want to be spammed with reports of an incident from customer service departments or it splashed all over social media.

The last core step is having a well oiled [post-mortem process](https://www.atlassian.com/incident-management/postmortem/templates). You want these to be entirely blameless and focused on finding ways these events don’t happen again/making the service more secure & stable. It’s once again, another ceremony that enforces a learning culture. It’s easy to do these and forget about them, so a reminder that it’s worth prioritising improvements and not just doing the theatre of reflecting on what went wrong but never doing anything about it.

## Retirement 
At some point your service might be worth wrapping up. In general if the service no longer adds value then it might be worth shutting down. This would probably be well choreographed far in advance as something will have replaced it or metrics indicate it no longer performs and is worth maintaining at all.
One noteworthy point related to retirement is communication to users. Ideally you want any remaining users to know that the service is being wrapped up and give advice on what to do next. The service doesn’t [need to just vanish](https://www.w3.org/Provider/Style/URI), you should try your best to redirect people to the best next destination.  
 
### Summary
Managing a live service is a wonderful phase to work in. You get time and space to refine your service and make users even happier while also lowering costs & making things run more efficiently. 
The biggest differences to previous phases is making your monitoring of performance more key to decision making, continue to find joy in making smaller incremental tweaks to your service then establishing stable foundations in place to make sure the service can run smoothly, despite incidents. 
